<!-- #########  USE RESPONSIBLY  #########   -->
<h2>Terms of Use</h2>
Have fun playing around with p-checker! This web application provides several tests for publication bias/p-hacking/indicators for data-dependent analyses, whatever term you prefer. Some of them are new, unpublished, and controversial to some extent; purpose of this app is to provide a unified place for trying out and comparing these methods. Please use the tests with care.
<br>
When you do an actual analysis, remember:
<ul>
<li>It is <i>not OK</i> to search for single papers which score low on a certain index ("cherry-picking"), and to single out these papers. Sampling variation applies to papers as well, and it can occur by chance that some rare combinations of results are found.</li>
<li>Always analyze papers with a defendable a priori inclusion criterion, e.g.: "All papers from an certain journal issue, which have more than 2 studies", or "The 10 most cited papers of a working group".
<li>Disclose the inclusion rule.</li>
<li>Take care what p-values can be included. p-curve, for example, assumes the independence of p-values. That means, you usually only extract one p-value per sample.</li>
<li>In general: RTFM of the tests you do!</li>
</ul>
<br>
I strongly recommend to read Simonsohn et al.'s (2014) <a href="http://www.p-curve.com">p-curve paper</a>. They have sensible recommendations and rules of thumb which papers and test statistics to include in an analysis.